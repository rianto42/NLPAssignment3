{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install Requirements","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n!pip install kagglehub\n!pip install langdetect\n!pip install transformers\n!pip install datasets\n!pip install -q wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T22:04:43.830007Z","iopub.execute_input":"2025-05-10T22:04:43.830237Z","iopub.status.idle":"2025-05-10T22:05:03.242392Z","shell.execute_reply.started":"2025-05-10T22:04:43.830214Z","shell.execute_reply":"2025-05-10T22:05:03.241386Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.11)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.1.31)\nCollecting langdetect\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\nBuilding wheels for collected packages: langdetect\n  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=582b87671c14271d285913c5cff538142769ae9f85782e6c8221641140eb5ace\n  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\nSuccessfully built langdetect\nInstalling collected packages: langdetect\nSuccessfully installed langdetect-1.0.9\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.12.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_API_KEY\"] = api_key\nos.environ[\"WANDB_PROJECT\"] = 'nlp'\n\nimport re\nimport wandb\nimport kagglehub\nimport numpy as np\nimport pandas as pd\nfrom langdetect import detect\nfrom datasets import DatasetDict, Dataset\nfrom transformers import DistilBertConfig, DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score,f1_score, precision_score, recall_score, confusion_matrix, classification_report\nimport torch\nimport torch.nn as nn\nfrom scipy.special import softmax\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T22:27:09.604795Z","iopub.execute_input":"2025-05-10T22:27:09.605408Z","iopub.status.idle":"2025-05-10T22:27:41.806758Z","shell.execute_reply.started":"2025-05-10T22:27:09.605385Z","shell.execute_reply":"2025-05-10T22:27:41.805952Z"}},"outputs":[{"name":"stderr","text":"2025-05-10 22:27:28.225953: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746916048.498451      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746916048.572398      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Data loading and preprocessing","metadata":{}},{"cell_type":"code","source":"# Download dataset\ndef load_dataset(used_column):\n    dataset_path = kagglehub.dataset_download(\"tobiasbueck/multilingual-customer-support-tickets\")\n    print(\"dataset downloaded to this path:\", dataset_path)\n    ds = pd.read_csv(os.path.join(dataset_path,'aa_dataset-tickets-multi-lang-5-2-50-version.csv'),usecols=used_column)\n    ds = ds.rename(columns={'queue': 'label'})\n    return ds\n\n# Cleansing text for unnecessary characters\ndef cleanse_text(text):\n    text = text.lower()\n    text = re.sub(r'<.*?>', '', text)\n    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n    text = re.sub(r\"\\b\\d{10,}\\b\", \"\", text)\n    text = text.replace('\\n', ' ').replace('\\r', ' ')\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# Load dataset and retain necessary columns\nused_column = ['body','queue','language']\nds = load_dataset(used_column)\n\n# Check null label value and discard the value\nif ds['label'].isnull().any():\n    print('There are some rows that has null label value, discard the rows')\n    ds = ds.dropna(subset=['label'])\n\n# Cleansing the text\nds[\"body\"] = ds['body'].apply(cleanse_text)\n\n# Discard non-english text\nds = ds[ds['language'] == 'en']\nds = ds.drop(columns=['language'])#.reset_index(drop=False)\n\n# Enumerate label\nlabel2id = {label: idx for idx, label in enumerate(ds['label'].unique())}\nds['label'] = ds['label'].map(label2id)\n\n# Reverse Enumerate label\nid2label = {v: k for k, v in label2id.items()}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Display Statistics","metadata":{}},{"cell_type":"code","source":"# Display labels\nprint('Unique value for category:', ds['label'].unique())\n\n# Check label distribution\nprint('Label freq:', ds['label'].value_counts(normalize=True) * 100)\n\n# Display total row number for each label\nprint('Label freq:', ds['label'].value_counts())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Splitting and Oversampling","metadata":{}},{"cell_type":"code","source":"def rebalance_dataframe_by_oversampling(df, label_col='label', random_state=42):\n    # Get the majority class size\n    class_counts = df[label_col].value_counts()\n    # Oversampling limit (All class or only minority class)\n    # max_count = class_counts.max()  # All class\n    max_count = int(df[label_col].value_counts().sum()/df[label_col].nunique())  # Only minority class\n\n    # List to hold oversampled DataFrames\n    balanced_dfs = []\n\n    for label, count in class_counts.items():\n        df_label = df[df[label_col] == label]\n        \n        if count < max_count:\n            df_upsampled = resample(\n                df_label,\n                replace=True, # oversampling\n                n_samples=max_count,\n                random_state=random_state\n            )\n        else:\n            df_upsampled = df_label\n\n        balanced_dfs.append(df_upsampled)\n\n    # Concat all balanced data\n    df_balanced = pd.concat(balanced_dfs).sample(frac=1, random_state=random_state).reset_index(drop=True)\n    return df_balanced","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds_ready = ds.copy()\n\n# Split the dataset\ntrain_ds, test_ds = train_test_split(ds_ready, test_size=0.3, random_state=42, stratify=ds_ready['label'])\n\n# Oversampling minority class\ntrain_ds = rebalance_dataframe_by_oversampling(train_ds)\nprint(\"Data distribution after oversampling:\\n\",train_ds['label'].value_counts())\n\n# Convert to Huggingface dataset object\ntrain_dataset = Dataset.from_pandas(train_ds.reset_index(drop=True))\ntest_dataset = Dataset.from_pandas(test_ds.reset_index(drop=True))\nready_dataset = DatasetDict({\n    \"train\": train_dataset,\n    \"test\": test_dataset\n})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"model_name = 'distilbert-base-uncased'\ntokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n\ndef tokenize_function(datas):\n    return tokenizer(datas[\"body\"], padding=\"max_length\", truncation=True, max_length=115)\n\ntokenized_dataset = ready_dataset.map(tokenize_function, batched=True)\ntokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\nnum_labels = train_ds['label'].nunique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Original model without focal loss\n# model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n\ndef compute_metrics(pred):\n    logits = pred.predictions\n    labels = pred.label_ids\n\n    probs = softmax(logits, axis=1) \n    preds = np.argmax(probs, axis=1)\n\n    # Accuracy\n    acc = accuracy_score(labels, preds)\n\n    # Macro\n    macro_f1 = f1_score(labels, preds, average='macro')\n    macro_precision = precision_score(labels, preds, average='macro', zero_division=0)\n    macro_recall = recall_score(labels, preds, average='macro', zero_division=0)\n\n    # Weighted\n    weighted_f1 = f1_score(labels, preds, average='weighted')\n    weighted_precision = precision_score(labels, preds, average='weighted', zero_division=0)\n    weighted_recall = recall_score(labels, preds, average='weighted', zero_division=0)\n\n    return {\n        'accuracy': acc,\n        'macro_f1': macro_f1,\n        'macro_precision': macro_precision,\n        'macro_recall': macro_recall,\n        'weighted_f1': weighted_f1,\n        'weighted_precision': weighted_precision,\n        'weighted_recall': weighted_recall\n    }\n\n# Get class weights based on label column\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_ds['label']),\n    y=train_ds['label']\n)\n\n# Convert to tensor\nclass_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=1.0):\n        super().__init__()\n        self.alpha = alpha  # Class weights\n        self.gamma = gamma\n\n    def forward(self, inputs, targets):\n        ce_loss = nn.CrossEntropyLoss(weight=self.alpha, reduction='none')(inputs, targets)\n        pt = torch.exp(-ce_loss)\n        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n        return focal_loss.mean()\n\nclass DistilBERTWithFocalLoss(DistilBertForSequenceClassification):\n    def __init__(self, config, class_weights, gamma=1.0):\n        super().__init__(config)\n        self.focal = FocalLoss(alpha=class_weights, gamma=gamma)\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.get(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        loss = self.focal(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\nconfig = DistilBertConfig.from_pretrained(\n    \"distilbert-base-uncased\", \n    # seq_classif_dropout=0.3, # Override default model classifier's dropout\n    num_labels=num_labels,\n    id2label=id2label,\n    label2id=label2id)\n\n# Implement focal loss\nmodel = DistilBERTWithFocalLoss.from_pretrained(\n    \"distilbert-base-uncased\",\n    config=config,\n    class_weights=class_weights_tensor\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model Structure\nprint(\"Model Structure:\\n\\n\")\nprint(model)\n\n# Model Configuration\nprint(\"Model Config:\\n\\n\")\nprint(model.config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Freeze all layer exclude the classifier\n# for param in model.distilbert.parameters():\n#     param.requires_grad = False\n\n# Check trainable model parameters\nfor name, param in model.named_parameters():\n    if param.requires_grad:\n        print(f\"Trainable: {name}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fine-Tuning Model","metadata":{}},{"cell_type":"code","source":"# Setup Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    num_train_epochs=10,\n    weight_decay=0.1,\n    learning_rate=2e-5, \n    logging_dir=\"./logs\",\n    logging_steps=5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"macro_f1\",\n    greater_is_better=True,\n    report_to=\"wandb\",\n    run_name=\"Version 11\",\n    warmup_steps=500,  # Add this\n    lr_scheduler_type=\"linear\",  # And this\n)\n\n# Trainer definition\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    compute_metrics=compute_metrics\n)\n\n# Do training\nprint(\"\\nFine-tuning\")\ntrainer.train()\n\n# Evaluate training result for best model found\nprint(\"\\nResult\")\ntrainer_results = trainer.evaluate()\nprint(trainer_results)\n\n# Display Classification Report\npredictions = trainer.predict(tokenized_dataset[\"test\"])\ny_true = predictions.label_ids\ny_pred = np.argmax(predictions.predictions, axis=1)\nlabel_names = [label for idx, label in sorted(id2label.items())]\nprint(classification_report(y_true, y_pred, target_names=label_names, digits=2))\n\n# Display Confusion Matrix\ncm = confusion_matrix(y_true, y_pred, normalize='true')\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n            xticklabels=label_names, yticklabels=label_names)\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix')\nplt.xticks(rotation=45)\nplt.yticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Stop wandb run\nwandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Optional Scripts","metadata":{}},{"cell_type":"code","source":"# To remove kaggle output directory when full\n!rm -rf /kaggle/working/*","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# To download the model\n\nimport shutil\nmodel.save_pretrained(\"./my_distilbert_model_version12\")\ntokenizer.save_pretrained(\"./my_distilbert_model_version12\")\nshutil.make_archive(\"my_distilbert_model_version12\", 'zip', \"./my_distilbert_model_version12\")\nfrom IPython.display import FileLink\nFileLink('my_distilbert_model_version12.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}